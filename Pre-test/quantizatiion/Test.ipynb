{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cumsum in module numpy:\n",
      "\n",
      "cumsum(a, axis=None, dtype=None, out=None)\n",
      "    Return the cumulative sum of the elements along a given axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array.\n",
      "    axis : int, optional\n",
      "        Axis along which the cumulative sum is computed. The default\n",
      "        (None) is to compute the cumsum over the flattened array.\n",
      "    dtype : dtype, optional\n",
      "        Type of the returned array and of the accumulator in which the\n",
      "        elements are summed.  If `dtype` is not specified, it defaults\n",
      "        to the dtype of `a`, unless `a` has an integer dtype with a\n",
      "        precision less than that of the default platform integer.  In\n",
      "        that case, the default platform integer is used.\n",
      "    out : ndarray, optional\n",
      "        Alternative output array in which to place the result. It must\n",
      "        have the same shape and buffer length as the expected output\n",
      "        but the type will be cast if necessary. See `ufuncs-output-type` for\n",
      "        more details.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    cumsum_along_axis : ndarray.\n",
      "        A new array holding the result is returned unless `out` is\n",
      "        specified, in which case a reference to `out` is returned. The\n",
      "        result has the same size as `a`, and the same shape as `a` if\n",
      "        `axis` is not None or `a` is a 1-d array.\n",
      "    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    sum : Sum array elements.\n",
      "    \n",
      "    trapz : Integration of array values using the composite trapezoidal rule.\n",
      "    \n",
      "    diff :  Calculate the n-th discrete difference along given axis.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Arithmetic is modular when using integer types, and no error is\n",
      "    raised on overflow.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1,2,3], [4,5,6]])\n",
      "    >>> a\n",
      "    array([[1, 2, 3],\n",
      "           [4, 5, 6]])\n",
      "    >>> np.cumsum(a)\n",
      "    array([ 1,  3,  6, 10, 15, 21])\n",
      "    >>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\n",
      "    array([  1.,   3.,   6.,  10.,  15.,  21.])\n",
      "    \n",
      "    >>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\n",
      "    array([[1, 2, 3],\n",
      "           [5, 7, 9]])\n",
      "    >>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\n",
      "    array([[ 1,  3,  6],\n",
      "           [ 4,  9, 15]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(1, 5)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  6, 10], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OLS in module statsmodels.regression.linear_model:\n",
      "\n",
      "class OLS(WLS)\n",
      " |  OLS(endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |  \n",
      " |  Ordinary Least Squares\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like\n",
      " |      A 1-d endogenous response variable. The dependent variable.\n",
      " |  exog : array_like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user. See\n",
      " |      :func:`statsmodels.tools.add_constant`.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  hasconst : None or bool\n",
      " |      Indicates whether the RHS includes a user-supplied constant. If True,\n",
      " |      a constant is not checked for and k_constant is set to 1 and all\n",
      " |      result statistics are calculated as if a constant is present. If\n",
      " |      False, a constant is not checked for and k_constant is set to 0.\n",
      " |  **kwargs\n",
      " |      Extra arguments that are used to set model properties when using the\n",
      " |      formula interface.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  weights : scalar\n",
      " |      Has an attribute weights = array(1.0) due to inheritance from WLS.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  WLS : Fit a linear model using Weighted Least Squares.\n",
      " |  GLS : Fit a linear model using Generalized Least Squares.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  No constant is added by the model unless you are using formulas.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import statsmodels.api as sm\n",
      " |  >>> Y = [1,3,4,5,2,3,4]\n",
      " |  >>> X = range(1,8)\n",
      " |  >>> X = sm.add_constant(X)\n",
      " |  >>> model = sm.OLS(Y,X)\n",
      " |  >>> results = model.fit()\n",
      " |  >>> results.params\n",
      " |  array([ 2.14285714,  0.25      ])\n",
      " |  \n",
      " |  >>> results.tvalues\n",
      " |  array([ 1.87867287,  0.98019606])\n",
      " |  \n",
      " |  >>> print(results.t_test([1, 0]))\n",
      " |  <T test: effect=array([ 2.14285714]), sd=array([[ 1.14062282]]), t=array([[ 1.87867287]]), p=array([[ 0.05953974]]), df_denom=5>\n",
      " |  >>> print(results.f_test(np.identity(2)))\n",
      " |  <F test: F=array([[ 19.46078431]]), p=[[ 0.00437251]], df_denom=5, df_num=2>\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OLS\n",
      " |      WLS\n",
      " |      RegressionModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_regularized(self, method='elastic_net', alpha=0.0, L1_wt=1.0, start_params=None, profile_scale=False, refit=False, **kwargs)\n",
      " |      Return a regularized fit to a linear regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str\n",
      " |          Either 'elastic_net' or 'sqrt_lasso'.\n",
      " |      alpha : scalar or array_like\n",
      " |          The penalty weight.  If a scalar, the same penalty weight\n",
      " |          applies to all variables in the model.  If a vector, it\n",
      " |          must have the same length as `params`, and contains a\n",
      " |          penalty weight for each coefficient.\n",
      " |      L1_wt : scalar\n",
      " |          The fraction of the penalty given to the L1 penalty term.\n",
      " |          Must be between 0 and 1 (inclusive).  If 0, the fit is a\n",
      " |          ridge fit, if 1 it is a lasso fit.\n",
      " |      start_params : array_like\n",
      " |          Starting values for ``params``.\n",
      " |      profile_scale : bool\n",
      " |          If True the penalized fit is computed using the profile\n",
      " |          (concentrated) log-likelihood for the Gaussian model.\n",
      " |          Otherwise the fit uses the residual sum of squares.\n",
      " |      refit : bool\n",
      " |          If True, the model is refit using only the variables that\n",
      " |          have non-zero coefficients in the regularized fit.  The\n",
      " |          refitted model is not regularized.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments that contain information used when\n",
      " |          constructing a model using the formula interface.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      statsmodels.base.elastic_net.RegularizedResults\n",
      " |          The regularized results.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The elastic net uses a combination of L1 and L2 penalties.\n",
      " |      The implementation closely follows the glmnet package in R.\n",
      " |      \n",
      " |      The function that is minimized is:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          0.5*RSS/n + alpha*((1-L1\\_wt)*|params|_2^2/2 + L1\\_wt*|params|_1)\n",
      " |      \n",
      " |      where RSS is the usual regression sum of squares, n is the\n",
      " |      sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2\n",
      " |      norms.\n",
      " |      \n",
      " |      For WLS and GLS, the RSS is calculated using the whitened endog and\n",
      " |      exog data.\n",
      " |      \n",
      " |      Post-estimation results are based on the same data used to\n",
      " |      select variables, hence may be subject to overfitting biases.\n",
      " |      \n",
      " |      The elastic_net method uses the following keyword arguments:\n",
      " |      \n",
      " |      maxiter : int\n",
      " |          Maximum number of iterations\n",
      " |      cnvrg_tol : float\n",
      " |          Convergence threshold for line searches\n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      The square root lasso approach is a variation of the Lasso\n",
      " |      that is largely self-tuning (the optimal tuning parameter\n",
      " |      does not depend on the standard deviation of the regression\n",
      " |      errors).  If the errors are Gaussian, the tuning parameter\n",
      " |      can be taken to be\n",
      " |      \n",
      " |      alpha = 1.1 * np.sqrt(n) * norm.ppf(1 - 0.05 / (2 * p))\n",
      " |      \n",
      " |      where n is the sample size and p is the number of predictors.\n",
      " |      \n",
      " |      The square root lasso uses the following keyword arguments:\n",
      " |      \n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      The cvxopt module is required to estimate model using the square root\n",
      " |      lasso.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [*] Friedman, Hastie, Tibshirani (2008).  Regularization paths for\n",
      " |         generalized linear models via coordinate descent.  Journal of\n",
      " |         Statistical Software 33(1), 1-22 Feb 2010.\n",
      " |      \n",
      " |      .. [*] A Belloni, V Chernozhukov, L Wang (2011).  Square-root Lasso:\n",
      " |         pivotal recovery of sparse signals via conic programming.\n",
      " |         Biometrika 98(4), 791-806. https://arxiv.org/pdf/1009.5689.pdf\n",
      " |  \n",
      " |  hessian(self, params, scale=None)\n",
      " |      Evaluate the Hessian function at a given point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameter vector at which the Hessian is computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          The Hessian matrix.\n",
      " |  \n",
      " |  hessian_factor(self, params, scale=None, observed=True)\n",
      " |      Calculate the weights for the Hessian.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The parameter at which Hessian is evaluated.\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      observed : bool\n",
      " |          If True, then the observed Hessian is returned. If false then the\n",
      " |          expected information matrix is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          A 1d weight vector used in the calculation of the Hessian.\n",
      " |          The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
      " |  \n",
      " |  loglike(self, params, scale=None)\n",
      " |      The likelihood function for the OLS model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The coefficients with which to estimate the log-likelihood.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The likelihood function evaluated at params.\n",
      " |  \n",
      " |  score(self, params, scale=None)\n",
      " |      Evaluate the score function at a given point.\n",
      " |      \n",
      " |      The score corresponds to the profile (concentrated)\n",
      " |      log-likelihood in which the scale parameter has been profiled\n",
      " |      out.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameter vector at which the score function is\n",
      " |          computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          The score vector.\n",
      " |  \n",
      " |  whiten(self, x)\n",
      " |      OLS model whitener does nothing.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          Data to be whitened.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array_like\n",
      " |          The input array unmodified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      OLS : Fit a linear model using Ordinary Least Squares.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RegressionModel:\n",
      " |  \n",
      " |  fit(self, method='pinv', cov_type='nonrobust', cov_kwds=None, use_t=None, **kwargs)\n",
      " |      Full fit of the model.\n",
      " |      \n",
      " |      The results include an estimate of covariance matrix, (whitened)\n",
      " |      residuals and an estimate of scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str, optional\n",
      " |          Can be \"pinv\", \"qr\".  \"pinv\" uses the Moore-Penrose pseudoinverse\n",
      " |          to solve the least squares problem. \"qr\" uses the QR\n",
      " |          factorization.\n",
      " |      cov_type : str, optional\n",
      " |          See `regression.linear_model.RegressionResults` for a description\n",
      " |          of the available covariance estimators.\n",
      " |      cov_kwds : list or None, optional\n",
      " |          See `linear_model.RegressionResults.get_robustcov_results` for a\n",
      " |          description required keywords for alternative covariance\n",
      " |          estimators.\n",
      " |      use_t : bool, optional\n",
      " |          Flag indicating to use the Student's t distribution when computing\n",
      " |          p-values.  Default behavior depends on cov_type. See\n",
      " |          `linear_model.RegressionResults.get_robustcov_results` for\n",
      " |          implementation details.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments that contain information used when\n",
      " |          constructing a model using the formula interface.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      RegressionResults\n",
      " |          The model estimation results.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      RegressionResults\n",
      " |          The results container.\n",
      " |      RegressionResults.get_robustcov_results\n",
      " |          A method to change the covariance estimator used when fitting the\n",
      " |          model.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The fit method uses the pseudoinverse of the design/exogenous variables\n",
      " |      to solve the least squares minimization.\n",
      " |  \n",
      " |  get_distribution(self, params, scale, exog=None, dist_class=None)\n",
      " |      Construct a random number generator for the predictive distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The model parameters (regression coefficients).\n",
      " |      scale : scalar\n",
      " |          The variance parameter.\n",
      " |      exog : array_like\n",
      " |          The predictor variable matrix.\n",
      " |      dist_class : class\n",
      " |          A random number generator class.  Must take 'loc' and 'scale'\n",
      " |          as arguments and return a random number generator implementing\n",
      " |          an ``rvs`` method for simulating random values. Defaults to normal.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      gen\n",
      " |          Frozen random number generator object with mean and variance\n",
      " |          determined by the fitted linear model.  Use the ``rvs`` method\n",
      " |          to generate random values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to the behavior of ``scipy.stats.distributions objects``,\n",
      " |      the returned random number generator must be called with\n",
      " |      ``gen.rvs(n)`` where ``n`` is the number of observations in\n",
      " |      the data set used to fit the model.  If any other value is\n",
      " |      used for ``n``, misleading results will be produced.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize model components.\n",
      " |  \n",
      " |  predict(self, params, exog=None)\n",
      " |      Return linear predicted values from a design matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Parameters of a linear model.\n",
      " |      exog : array_like, optional\n",
      " |          Design / exogenous data. Model exog is used if None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array_like\n",
      " |          An array of fitted values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the model has not yet been fit, params is not optional.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RegressionModel:\n",
      " |  \n",
      " |  df_model\n",
      " |      The model degree of freedom.\n",
      " |      \n",
      " |      The dof is defined as the rank of the regressor matrix minus 1 if a\n",
      " |      constant is included.\n",
      " |  \n",
      " |  df_resid\n",
      " |      The residual degree of freedom.\n",
      " |      \n",
      " |      The dof is defined as the number of observations minus the rank of\n",
      " |      the regressor matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model.\n",
      " |      \n",
      " |      Returns -1 * Hessian of the log-likelihood evaluated at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The model parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model.\n",
      " |      data : array_like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array_like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`.\n",
      " |      drop_cols : array_like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      *args\n",
      " |          Additional positional argument that are passed to the model.\n",
      " |      **kwargs\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model\n",
      " |          The model instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables.\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(regression.linear_model.OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {}\n",
    "dic[1] = 'a'\n",
    "dic[2] = 'b'\n",
    "\n",
    "size = len(dic)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module numpy.polynomial._polybase:\n",
      "\n",
      "fit(x, y, deg, domain=None, rcond=None, full=False, w=None, window=None) method of abc.ABCMeta instance\n",
      "    Least squares fit to data.\n",
      "    \n",
      "    Return a series instance that is the least squares fit to the data\n",
      "    `y` sampled at `x`. The domain of the returned instance can be\n",
      "    specified and this will often result in a superior fit with less\n",
      "    chance of ill conditioning.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like, shape (M,)\n",
      "        x-coordinates of the M sample points ``(x[i], y[i])``.\n",
      "    y : array_like, shape (M,) or (M, K)\n",
      "        y-coordinates of the sample points. Several data sets of sample\n",
      "        points sharing the same x-coordinates can be fitted at once by\n",
      "        passing in a 2D-array that contains one dataset per column.\n",
      "    deg : int or 1-D array_like\n",
      "        Degree(s) of the fitting polynomials. If `deg` is a single integer\n",
      "        all terms up to and including the `deg`'th term are included in the\n",
      "        fit. For NumPy versions >= 1.11.0 a list of integers specifying the\n",
      "        degrees of the terms to include may be used instead.\n",
      "    domain : {None, [beg, end], []}, optional\n",
      "        Domain to use for the returned series. If ``None``,\n",
      "        then a minimal domain that covers the points `x` is chosen.  If\n",
      "        ``[]`` the class domain is used. The default value was the\n",
      "        class domain in NumPy 1.4 and ``None`` in later versions.\n",
      "        The ``[]`` option was added in numpy 1.5.0.\n",
      "    rcond : float, optional\n",
      "        Relative condition number of the fit. Singular values smaller\n",
      "        than this relative to the largest singular value will be\n",
      "        ignored. The default value is len(x)*eps, where eps is the\n",
      "        relative precision of the float type, about 2e-16 in most\n",
      "        cases.\n",
      "    full : bool, optional\n",
      "        Switch determining nature of return value. When it is False\n",
      "        (the default) just the coefficients are returned, when True\n",
      "        diagnostic information from the singular value decomposition is\n",
      "        also returned.\n",
      "    w : array_like, shape (M,), optional\n",
      "        Weights. If not None the contribution of each point\n",
      "        ``(x[i],y[i])`` to the fit is weighted by `w[i]`. Ideally the\n",
      "        weights are chosen so that the errors of the products\n",
      "        ``w[i]*y[i]`` all have the same variance.  The default value is\n",
      "        None.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "    window : {[beg, end]}, optional\n",
      "        Window to use for the returned series. The default\n",
      "        value is the default class domain\n",
      "    \n",
      "        .. versionadded:: 1.6.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    new_series : series\n",
      "        A series that represents the least squares fit to the data and\n",
      "        has the domain and window specified in the call. If the\n",
      "        coefficients for the unscaled and unshifted basis polynomials are\n",
      "        of interest, do ``new_series.convert().coef``.\n",
      "    \n",
      "    [resid, rank, sv, rcond] : list\n",
      "        These values are only returned if `full` = True\n",
      "    \n",
      "        resid -- sum of squared residuals of the least squares fit\n",
      "        rank -- the numerical rank of the scaled Vandermonde matrix\n",
      "        sv -- singular values of the scaled Vandermonde matrix\n",
      "        rcond -- value of `rcond`.\n",
      "    \n",
      "        For more details, see `linalg.lstsq`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.polynomial.Chebyshev.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.5: 'a', 1.8: 'b', 1.3: 'c', 1.1: 'd', 1.2: 'e', 1.4: 'g', 1.9: 'xyz'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {}\n",
    "dic[1.5] = 'a'\n",
    "dic[1.8] = 'b'\n",
    "dic[1.3] = 'c'\n",
    "dic[1.1] = 'd'\n",
    "dic[1.2] = 'e'\n",
    "dic[1.9] = 'f'\n",
    "dic[1.4] = 'g'\n",
    "\n",
    "max(dic.keys())\n",
    "\n",
    "dic.pop(1.9)\n",
    "dic[1.9] = 'xyz'\n",
    "sorted(dic.keys())\n",
    "dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KLoading sha stock data |#######                         | 400/1613oading sha stock data |                                | 29/1613Loading sha stock data |                                | 34/1613Loading sha stock data |                                | 47/1613Loading sha stock data |                                | 49/1613Loading sha stock data |#                               | 52/1613Loading sha stock data |#                               | 56/1613Loading sha stock data |#                               | 59/1613Loading sha stock data |#                               | 65/1613Loading sha stock data |#                               | 66/1613Loading sha stock data |#                               | 67/1613Loading sha stock data |#                               | 87/1613Loading sha stock data |#                               | 88/1613Loading sha stock data |#                               | 89/1613Loading sha stock data |#                               | 90/1613Loading sha stock data |#                               | 95/1613Loading sha stock data |#                               | 96/1613Loading sha stock data |#                               | 97/1613Loading sha stock data |#                               | 99/1613Loading sha stock data |#                               | 100/1613Loading sha stock data |##                              | 101/1613Loading sha stock data |##                              | 102/1613Loading sha stock data |##                              | 104/1613Loading sha stock data |##                              | 105/1613Loading sha stock data |##                              | 106/1613Loading sha stock data |##                              | 107/1613Loading sha stock data |##                              | 108/1613Loading sha stock data |##                              | 110/1613Loading sha stock data |##                              | 111/1613Loading sha stock data |##                              | 112/1613Loading sha stock data |##                              | 113/1613Loading sha stock data |##                              | 114/1613Loading sha stock data |##                              | 116/1613Loading sha stock data |##                              | 117/1613Loading sha stock data |##                              | 119/1613Loading sha stock data |##                              | 120/1613Loading sha stock data |##                              | 121/1613Loading sha stock data |##                              | 122/1613Loading sha stock data |##                              | 123/1613Loading sha stock data |##                              | 124/1613Loading sha stock data |##                              | 126/1613Loading sha stock data |##                              | 127/1613Loading sha stock data |##                              | 128/1613Loading sha stock data |##                              | 130/1613Loading sha stock data |##                              | 131/1613Loading sha stock data |##                              | 133/1613Loading sha stock data |##                              | 135/1613Loading sha stock data |##                              | 137/1613Loading sha stock data |##                              | 138/1613Loading sha stock data |##                              | 139/1613Loading sha stock data |##                              | 140/1613Loading sha stock data |##                              | 142/1613Loading sha stock data |##                              | 143/1613Loading sha stock data |##                              | 144/1613Loading sha stock data |##                              | 145/1613Loading sha stock data |##                              | 146/1613Loading sha stock data |##                              | 148/1613Loading sha stock data |##                              | 150/1613Loading sha stock data |##                              | 151/1613Loading sha stock data |###                             | 152/1613Loading sha stock data |###                             | 191/1613Loading sha stock data |###                             | 200/1613Loading sha stock data |####                            | 202/1613Loading sha stock data |####                            | 204/1613Loading sha stock data |####                            | 205/1613Loading sha stock data |####                            | 213/1613Loading sha stock data |####                            | 217/1613Loading sha stock data |####                            | 218/1613Loading sha stock data |####                            | 219/1613Loading sha stock data |####                            | 220/1613Loading sha stock data |####                            | 221/1613Loading sha stock data |####                            | 222/1613Loading sha stock data |####                            | 223/1613Loading sha stock data |####                            | 224/1613Loading sha stock data |####                            | 225/1613Loading sha stock data |####                            | 226/1613Loading sha stock data |####                            | 232/1613Loading sha stock data |####                            | 233/1613Loading sha stock data |####                            | 237/1613Loading sha stock data |#####                           | 261/1613Loading sha stock data |#####                           | 262/1613Loading sha stock data |#####                           | 279/1613Loading sha stock data |#####                           | 282/1613Loading sha stock data |#####                           | 290/1613Loading sha stock data |#####                           | 294/1613Loading sha stock data |#####                           | 295/1613Loading sha stock data |#####                           | 296/1613Loading sha stock data |######                          | 320/1613Loading sha stock data |######                          | 325/1613Loading sha stock data |######                          | 331/1613Loading sha stock data |######                          | 335/1613Loading sha stock data |######                          | 336/1613Loading sha stock data |######                          | 337/1613Loading sha stock data |######                          | 339/1613Loading sha stock data |######                          | 340/1613Loading sha stock data |######                          | 341/1613Loading sha stock data |######                          | 345/1613Loading sha stock data |######                          | 347/1613Loading sha stock data |######                          | 349/1613Loading sha stock data |#######                         | 356/1613Loading sha stock data |#######                         | 358/1613Loading sha stock data |#######                         | 359/1613Loading sha stock data |#######                         | 360/1613Loading sha stock data |#######                         | 363/1613Loading sha stock data |#######                         | 365/1613Loading sha stock data |#######                         | 366/1613Loading sha stock data |#######                         | 367/1613Loading sha stock data |#######                         | 371/1613Loading sha stock data |#######                         | 375/1613Loading sha stock data |#######                         | 378/1613Loading sha stock data |#######                         | 381/1613Loading sha stock data |#######                         | 383/1613Loading sha stock data |#######                         | 384/1613Loading sha stock data |#######                         | 385/1613Loading sha stock data |#######                         | 389/1613Loading sha stock data |#######                         | 390/1613Loading sha stock data |#######                         | 391/1613Loading sha stock data |#######                         | 392/1613Loading sha stock data |#######                         | 393/1613Loading sha stock data |#######                         | 394/1613Loading sha stock data |#######                         | 399/1613Loading sha stock data |#######                         | 401/1613Loading sha stock data |#######                         | 402/1613Loading sha stock data |#######                         | 403/1613Loading sha stock data |########                        | 404/1613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KLoading sha stock data |############                    | 645/1613Loading sha stock data |########                        | 407/1613Loading sha stock data |########                        | 409/1613Loading sha stock data |########                        | 410/1613Loading sha stock data |########                        | 411/1613Loading sha stock data |########                        | 412/1613Loading sha stock data |########                        | 413/1613Loading sha stock data |########                        | 414/1613Loading sha stock data |########                        | 415/1613Loading sha stock data |########                        | 416/1613Loading sha stock data |########                        | 417/1613Loading sha stock data |########                        | 418/1613Loading sha stock data |########                        | 419/1613Loading sha stock data |########                        | 420/1613Loading sha stock data |########                        | 421/1613Loading sha stock data |########                        | 422/1613Loading sha stock data |########                        | 423/1613Loading sha stock data |########                        | 424/1613Loading sha stock data |########                        | 425/1613Loading sha stock data |########                        | 427/1613Loading sha stock data |########                        | 428/1613Loading sha stock data |########                        | 429/1613Loading sha stock data |########                        | 431/1613Loading sha stock data |########                        | 432/1613Loading sha stock data |########                        | 433/1613Loading sha stock data |########                        | 434/1613Loading sha stock data |########                        | 435/1613Loading sha stock data |########                        | 436/1613Loading sha stock data |########                        | 438/1613Loading sha stock data |########                        | 439/1613Loading sha stock data |########                        | 440/1613Loading sha stock data |########                        | 441/1613Loading sha stock data |########                        | 442/1613Loading sha stock data |########                        | 443/1613Loading sha stock data |########                        | 444/1613Loading sha stock data |########                        | 446/1613Loading sha stock data |########                        | 448/1613Loading sha stock data |########                        | 451/1613Loading sha stock data |########                        | 453/1613Loading sha stock data |#########                       | 454/1613Loading sha stock data |#########                       | 455/1613Loading sha stock data |#########                       | 462/1613Loading sha stock data |#########                       | 463/1613Loading sha stock data |#########                       | 464/1613Loading sha stock data |#########                       | 465/1613Loading sha stock data |#########                       | 469/1613Loading sha stock data |#########                       | 470/1613Loading sha stock data |#########                       | 471/1613Loading sha stock data |#########                       | 477/1613Loading sha stock data |#########                       | 479/1613Loading sha stock data |#########                       | 481/1613Loading sha stock data |#########                       | 482/1613Loading sha stock data |#########                       | 485/1613Loading sha stock data |#########                       | 487/1613Loading sha stock data |#########                       | 488/1613Loading sha stock data |#########                       | 490/1613Loading sha stock data |#########                       | 493/1613Loading sha stock data |#########                       | 496/1613Loading sha stock data |#########                       | 498/1613Loading sha stock data |##########                      | 506/1613Loading sha stock data |##########                      | 508/1613Loading sha stock data |##########                      | 510/1613Loading sha stock data |##########                      | 524/1613Loading sha stock data |##########                      | 531/1613Loading sha stock data |##########                      | 534/1613Loading sha stock data |##########                      | 537/1613Loading sha stock data |##########                      | 542/1613Loading sha stock data |##########                      | 544/1613Loading sha stock data |##########                      | 547/1613Loading sha stock data |###########                     | 558/1613Loading sha stock data |###########                     | 562/1613Loading sha stock data |###########                     | 563/1613Loading sha stock data |###########                     | 565/1613Loading sha stock data |###########                     | 570/1613Loading sha stock data |###########                     | 579/1613Loading sha stock data |###########                     | 585/1613Loading sha stock data |###########                     | 586/1613Loading sha stock data |###########                     | 588/1613Loading sha stock data |###########                     | 590/1613Loading sha stock data |###########                     | 591/1613Loading sha stock data |###########                     | 593/1613Loading sha stock data |###########                     | 594/1613Loading sha stock data |###########                     | 596/1613Loading sha stock data |###########                     | 599/1613Loading sha stock data |###########                     | 601/1613Loading sha stock data |###########                     | 602/1613Loading sha stock data |###########                     | 603/1613Loading sha stock data |###########                     | 604/1613Loading sha stock data |############                    | 605/1613Loading sha stock data |############                    | 606/1613Loading sha stock data |############                    | 607/1613Loading sha stock data |############                    | 608/1613Loading sha stock data |############                    | 609/1613Loading sha stock data |############                    | 611/1613Loading sha stock data |############                    | 612/1613Loading sha stock data |############                    | 613/1613Loading sha stock data |############                    | 614/1613Loading sha stock data |############                    | 615/1613Loading sha stock data |############                    | 616/1613Loading sha stock data |############                    | 617/1613Loading sha stock data |############                    | 618/1613Loading sha stock data |############                    | 624/1613Loading sha stock data |############                    | 626/1613Loading sha stock data |############                    | 627/1613Loading sha stock data |############                    | 628/1613Loading sha stock data |############                    | 630/1613Loading sha stock data |############                    | 631/1613Loading sha stock data |############                    | 633/1613Loading sha stock data |############                    | 634/1613Loading sha stock data |############                    | 635/1613Loading sha stock data |############                    | 636/1613Loading sha stock data |############                    | 637/1613Loading sha stock data |############                    | 638/1613Loading sha stock data |############                    | 639/1613Loading sha stock data |############                    | 640/1613Loading sha stock data |############                    | 641/1613Loading sha stock data |############                    | 642/1613Loading sha stock data |############                    | 643/1613Loading sha stock data |############                    | 646/1613Loading sha stock data |############                    | 647/1613Loading sha stock data |############                    | 648/1613Loading sha stock data |############                    | 649/1613Loading sha stock data |############                    | 650/1613Loading sha stock data |############                    | 651/1613Loading sha stock data |############                    | 652/1613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KLoading sha stock data |############################    | 1453/1613oading sha stock data |#############                   | 660/1613Loading sha stock data |#############                   | 665/1613Loading sha stock data |#############                   | 666/1613Loading sha stock data |#############                   | 668/1613Loading sha stock data |#############                   | 670/1613Loading sha stock data |#############                   | 682/1613Loading sha stock data |#############                   | 686/1613Loading sha stock data |#############                   | 694/1613Loading sha stock data |#############                   | 698/1613Loading sha stock data |##############                  | 713/1613Loading sha stock data |##############                  | 728/1613Loading sha stock data |##############                  | 743/1613Loading sha stock data |##############                  | 744/1613Loading sha stock data |##############                  | 747/1613Loading sha stock data |###############                 | 757/1613Loading sha stock data |###############                 | 758/1613Loading sha stock data |###############                 | 771/1613Loading sha stock data |###############                 | 788/1613Loading sha stock data |################                | 811/1613Loading sha stock data |################                | 821/1613Loading sha stock data |################                | 824/1613Loading sha stock data |################                | 828/1613Loading sha stock data |################                | 853/1613Loading sha stock data |#################               | 889/1613Loading sha stock data |#################               | 905/1613Loading sha stock data |##################              | 930/1613Loading sha stock data |##################              | 933/1613Loading sha stock data |##################              | 938/1613Loading sha stock data |##################              | 939/1613Loading sha stock data |##################              | 946/1613Loading sha stock data |##################              | 951/1613Loading sha stock data |##################              | 953/1613Loading sha stock data |##################              | 956/1613Loading sha stock data |###################             | 976/1613Loading sha stock data |###################             | 977/1613Loading sha stock data |###################             | 978/1613Loading sha stock data |###################             | 980/1613Loading sha stock data |###################             | 986/1613Loading sha stock data |###################             | 994/1613Loading sha stock data |###################             | 1005/1613Loading sha stock data |####################            | 1009/1613Loading sha stock data |####################            | 1012/1613Loading sha stock data |####################            | 1020/1613Loading sha stock data |####################            | 1032/1613Loading sha stock data |#####################           | 1067/1613Loading sha stock data |#####################           | 1095/1613Loading sha stock data |######################          | 1113/1613Loading sha stock data |######################          | 1116/1613Loading sha stock data |######################          | 1125/1613Loading sha stock data |######################          | 1142/1613Loading sha stock data |######################          | 1145/1613Loading sha stock data |#######################         | 1180/1613Loading sha stock data |#######################         | 1194/1613Loading sha stock data |########################        | 1212/1613Loading sha stock data |########################        | 1231/1613Loading sha stock data |########################        | 1236/1613Loading sha stock data |########################        | 1239/1613Loading sha stock data |########################        | 1256/1613Loading sha stock data |#########################       | 1269/1613Loading sha stock data |#########################       | 1285/1613Loading sha stock data |#########################       | 1292/1613Loading sha stock data |#########################       | 1309/1613Loading sha stock data |#########################       | 1310/1613Loading sha stock data |##########################      | 1312/1613Loading sha stock data |##########################      | 1315/1613Loading sha stock data |##########################      | 1318/1613Loading sha stock data |##########################      | 1320/1613Loading sha stock data |##########################      | 1321/1613Loading sha stock data |##########################      | 1324/1613Loading sha stock data |##########################      | 1330/1613Loading sha stock data |##########################      | 1332/1613Loading sha stock data |##########################      | 1342/1613Loading sha stock data |##########################      | 1356/1613Loading sha stock data |##########################      | 1360/1613Loading sha stock data |###########################     | 1362/1613Loading sha stock data |###########################     | 1363/1613Loading sha stock data |###########################     | 1366/1613Loading sha stock data |###########################     | 1367/1613Loading sha stock data |###########################     | 1368/1613Loading sha stock data |###########################     | 1369/1613Loading sha stock data |###########################     | 1370/1613Loading sha stock data |###########################     | 1371/1613Loading sha stock data |###########################     | 1372/1613Loading sha stock data |###########################     | 1375/1613Loading sha stock data |###########################     | 1378/1613Loading sha stock data |###########################     | 1380/1613Loading sha stock data |###########################     | 1382/1613Loading sha stock data |###########################     | 1383/1613Loading sha stock data |###########################     | 1386/1613Loading sha stock data |###########################     | 1387/1613Loading sha stock data |###########################     | 1390/1613Loading sha stock data |###########################     | 1391/1613Loading sha stock data |###########################     | 1392/1613Loading sha stock data |###########################     | 1393/1613Loading sha stock data |###########################     | 1394/1613Loading sha stock data |###########################     | 1395/1613Loading sha stock data |###########################     | 1396/1613Loading sha stock data |###########################     | 1397/1613Loading sha stock data |###########################     | 1398/1613Loading sha stock data |###########################     | 1399/1613Loading sha stock data |###########################     | 1400/1613Loading sha stock data |###########################     | 1401/1613Loading sha stock data |###########################     | 1404/1613Loading sha stock data |###########################     | 1405/1613Loading sha stock data |###########################     | 1408/1613Loading sha stock data |###########################     | 1409/1613Loading sha stock data |###########################     | 1410/1613Loading sha stock data |###########################     | 1411/1613Loading sha stock data |############################    | 1415/1613Loading sha stock data |############################    | 1421/1613Loading sha stock data |############################    | 1422/1613Loading sha stock data |############################    | 1423/1613Loading sha stock data |############################    | 1424/1613Loading sha stock data |############################    | 1426/1613Loading sha stock data |############################    | 1427/1613Loading sha stock data |############################    | 1428/1613Loading sha stock data |############################    | 1429/1613Loading sha stock data |############################    | 1438/1613Loading sha stock data |############################    | 1440/1613Loading sha stock data |############################    | 1443/1613Loading sha stock data |############################    | 1454/1613Loading sha stock data |############################    | 1455/1613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KLoading sha stock data |################################| 1613/1613Loading sha stock data |#############################   | 1463/1613Loading sha stock data |#############################   | 1471/1613Loading sha stock data |#############################   | 1473/1613Loading sha stock data |#############################   | 1476/1613Loading sha stock data |#############################   | 1480/1613Loading sha stock data |#############################   | 1481/1613Loading sha stock data |#############################   | 1482/1613Loading sha stock data |#############################   | 1483/1613Loading sha stock data |#############################   | 1495/1613Loading sha stock data |#############################   | 1511/1613Loading sha stock data |##############################  | 1518/1613Loading sha stock data |##############################  | 1522/1613Loading sha stock data |##############################  | 1525/1613Loading sha stock data |##############################  | 1529/1613Loading sha stock data |############################### | 1566/1613Loading sha stock data |############################### | 1575/1613Loading sha stock data |############################### | 1579/1613Loading sha stock data |############################### | 1597/1613Loading sha stock data |############################### | 1607/1613\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%run init.ipynb\n",
    "%run load_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run init.ipynb   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre] Going to calculate 1 stock sample(s), start from SH#601608 to SH#601608\n",
      "SH#601696 37 938\n",
      "SH#603095 3 1070\n",
      "SH#603221 19 1134\n",
      "SH#603353 9 1200\n",
      "SH#603719 39 1359\n",
      "SH#603948 34 1471\n",
      "SH#603949 28 1472\n",
      "SH#688051 20 1533\n",
      "SH#688085 7 1540\n",
      "SH#688086 37 1541\n",
      "SH#688096 7 1545\n",
      "SH#688189 16 1569\n",
      "SH#688222 2 1577\n",
      "SH#688228 15 1578\n",
      "SH#688396 36 1599\n"
     ]
    }
   ],
   "source": [
    "def pre_test(offset, nstock):\n",
    "    stock_index = 0\n",
    "    stock_cnt = 0\n",
    "    first_stock_id = ''\n",
    "    last_stock_id = ''\n",
    "    current_stock_id = ''\n",
    "    for stock_type in sorted(Global_DB):\n",
    "        for stock_id in sorted(Global_DB[stock_type]):\n",
    "            df = Global_DB[stock_type][stock_id]\n",
    "            try:\n",
    "                current_stock_id = df['Name'][0]\n",
    "            except:\n",
    "                return df\n",
    "            \n",
    "            if stock_index == offset:\n",
    "                first_stock_id = current_stock_id\n",
    "\n",
    "            if stock_index >= offset and stock_index < offset + nstock:\n",
    "                stock_cnt += 1\n",
    "\n",
    "            if stock_index == offset + nstock - 1:\n",
    "                last_stock_id = current_stock_id\n",
    "                break\n",
    "\n",
    "            stock_index += 1\n",
    "\n",
    "    if last_stock_id == '':\n",
    "        last_stock_id = current_stock_id\n",
    "\n",
    "    print('[Pre] Going to calculate {} stock sample(s), start from {} to {}'.format(stock_cnt, first_stock_id, last_stock_id))\n",
    "\n",
    "    return None\n",
    "\n",
    "df = pre_test(921, 1)\n",
    "df\n",
    "\n",
    "def func():\n",
    "    cnt = 0\n",
    "    for stock_type in sorted(Global_DB):\n",
    "        for stock_id in sorted(Global_DB[stock_type]):\n",
    "            df = Global_DB[stock_type][stock_id]\n",
    "            if df.shape[0] < 40:\n",
    "                print('{} {} {}'.format(df['Name'][0], df.shape[0], cnt))\n",
    "                \n",
    "            cnt += 1\n",
    "                      \n",
    "func()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
